{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fd1675be6a320e69d8e7bb772717a3faf4f7820"
      },
      "cell_type": "code",
      "source": "#! pip install pandas\n#! pip install pydicom\n#! pip install seaborn\n#! pip install memory_profiler\n%load_ext memory_profiler\n\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom skimage.transform import resize\nfrom skimage.exposure import equalize_hist\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, clear_output\nimport numpy as np\n%matplotlib nbagg\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(sns.dark_palette(\"purple\"))\n\nimport torch\ncuda = torch.cuda.is_available()\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom functools import reduce\n\nimport torch.nn as nn\nfrom torch.nn.functional import softplus\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom itertools import cycle\nimport operator\nfrom torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU, ELU, ConvTranspose2d, MaxUnpool2d, Softmax, Sigmoid, Softsign\nfrom torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, dropout, dropout2d, interpolate\nimport time",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "3f518499954d2ad60289c06138d796a17a6f5aee"
      },
      "cell_type": "code",
      "source": "######### Loading data #########\n\nKAGGLE = True\n \n# display label format\nif KAGGLE:\n    #df = pd.read_csv('../input/stage_2_train_labels.csv')\n    df = pd.read_csv('../input/stage_2_detailed_class_info.csv')\n    df.rename(columns={'class': 'Target'}, inplace=True)\n    mapping = {'Normal': 0, 'Lung Opacity': 1, 'No Lung Opacity / Not Normal': 2}\n    df = df.replace({'Target': mapping})\n    df = df[ df['Target']!=2 ]\n    df = df.reset_index(drop=True)\nelse:\n    df = pd.read_csv('data/stage_1_train_labels.csv')\n\n\n# Parameters\nbatch_size = 64\nNo_train_samples = 64*80\nNo_test_samples = 2048\nclasses = [0,1]   #['Normal , 'Lung Opacity']  \nlabels_per_class = 64*40 # Specify how many labelled examples we want per digit class\nNo_train_labelled_samples = labels_per_class*len(classes)\n\nNo = 4\npatientId = df['patientId'][No]\nif KAGGLE:\n    dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId\nelse:\n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId\ndcm_data = pydicom.read_file(dcm_file)\n#print(df.iloc[No])\n\n\nIMG_SIZE = 112\nimg_dimension = [IMG_SIZE,IMG_SIZE] # New size of xray images. \nunq, idx = np.unique(df['patientId'], return_index = True) # Get only unique entrances from the provided data (some patients occur multiple times)\n\nprint_every = 100\nDo_img_eq = True\n\n######### Loading UNLABELED TRAINING data #########\nTarget = []\nImage = []\nelapsed_time = 0\ntic = time.clock()\n\nprint(\"Loading training images: 0 /\", No_train_samples)\nfor i in range(0,No_train_samples):\n    Target.append(df.Target[idx[i]]) # Get label  \n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i,\"/\",No_train_samples,  \"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();   \nprint(\"Train data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nTarget_train = Target\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n\n######### Loading TEST data #########\nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\nfor i in range(0,No_test_samples):\n    Target.append(df.Target[idx[No_train_samples+i]]) # Get label  \n    patientId = df['patientId'][idx[No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_test_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_test_samples-i)//print_every)%60)\n            print(\"Test data loaded:\", i, \"/\",No_test_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n\nprint(\"Test data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nTarget_test = Target\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntest_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n\n######### Loading LABELED TRAINING data #########\nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\ncount_unlabelled = 0\ncount_labelled = 0\ni = 0\nwhile count_labelled<labels_per_class or count_unlabelled<labels_per_class:\n#for i in range(0,No_train_labelled_samples):\n    Target.append(df.Target[idx[No_test_samples+No_train_samples+i]]) # Get label  \n\n    if Target[i]==1:\n        count_labelled = count_labelled + 1\n    else:\n        count_unlabelled = count_unlabelled + 1\n        \n    patientId = df['patientId'][idx[No_test_samples+No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)%60)\n            print(\"Labelled data loaded:\", i, \"/\",No_train_labelled_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n    i = i + 1\n\nprint(\"Labelled data loaded:\", i+1)\n\n\ndef uniform_stratified_sampler(labels, n=None):\n    \"\"\"\n    Stratified sampler that distributes labels uniformly by\n    sampling at most n data points per class\n    \"\"\"\n    from functools import reduce\n    # Only choose digits in n_labels\n    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n\n    # Ensure uniform distribution of labels\n    np.random.shuffle(indices)\n    indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in classes])\n\n    indices = torch.from_numpy(indices)\n    sampler = SubsetRandomSampler(indices)\n    return sampler\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader_labelled = DataLoader(loader, batch_size=batch_size,\n                     sampler=uniform_stratified_sampler(Target, labels_per_class))\n\ndel df, idx, unq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading training images: 0 / 5120\nData loaded: 100 / 5120     Remaining time:  5 : 0\nData loaded: 200 / 5120     Remaining time:  4 : 5\nData loaded: 300 / 5120     Remaining time:  4 : 0\nData loaded: 400 / 5120     Remaining time:  3 : 55\nData loaded: 500 / 5120     Remaining time:  3 : 50\nData loaded: 600 / 5120     Remaining time:  3 : 45\nData loaded: 700 / 5120     Remaining time:  3 : 40\nData loaded: 800 / 5120     Remaining time:  3 : 35\nData loaded: 900 / 5120     Remaining time:  3 : 30\nData loaded: 1000 / 5120     Remaining time:  3 : 25\nData loaded: 1100 / 5120     Remaining time:  3 : 20\nData loaded: 1200 / 5120     Remaining time:  3 : 15\nData loaded: 1300 / 5120     Remaining time:  3 : 10\nData loaded: 1400 / 5120     Remaining time:  3 : 5\nData loaded: 1500 / 5120     Remaining time:  3 : 0\nData loaded: 1600 / 5120     Remaining time:  2 : 55\nData loaded: 1700 / 5120     Remaining time:  2 : 50\nData loaded: 1800 / 5120     Remaining time:  2 : 45\nData loaded: 1900 / 5120     Remaining time:  2 : 40\nData loaded: 2000 / 5120     Remaining time:  2 : 35\nData loaded: 2100 / 5120     Remaining time:  2 : 30\nData loaded: 2200 / 5120     Remaining time:  2 : 25\nData loaded: 2300 / 5120     Remaining time:  2 : 20\nData loaded: 2400 / 5120     Remaining time:  2 : 15\nData loaded: 2500 / 5120     Remaining time:  2 : 10\nData loaded: 2600 / 5120     Remaining time:  2 : 5\nData loaded: 2700 / 5120     Remaining time:  2 : 0\nData loaded: 2800 / 5120     Remaining time:  1 : 55\nData loaded: 2900 / 5120     Remaining time:  1 : 50\nData loaded: 3000 / 5120     Remaining time:  1 : 45\nData loaded: 3100 / 5120     Remaining time:  1 : 40\nData loaded: 3200 / 5120     Remaining time:  1 : 35\nData loaded: 3300 / 5120     Remaining time:  1 : 30\nData loaded: 3400 / 5120     Remaining time:  1 : 25\nData loaded: 3500 / 5120     Remaining time:  1 : 20\nData loaded: 3600 / 5120     Remaining time:  1 : 15\nData loaded: 3700 / 5120     Remaining time:  1 : 10\nData loaded: 3800 / 5120     Remaining time:  1 : 5\nData loaded: 3900 / 5120     Remaining time:  1 : 0\nData loaded: 4000 / 5120     Remaining time:  0 : 55\nData loaded: 4100 / 5120     Remaining time:  0 : 50\nData loaded: 4200 / 5120     Remaining time:  0 : 45\nData loaded: 4300 / 5120     Remaining time:  0 : 40\nData loaded: 4400 / 5120     Remaining time:  0 : 35\nData loaded: 4500 / 5120     Remaining time:  0 : 30\nData loaded: 4600 / 5120     Remaining time:  0 : 25\nData loaded: 4700 / 5120     Remaining time:  0 : 20\nData loaded: 4800 / 5120     Remaining time:  0 : 15\nData loaded: 4900 / 5120     Remaining time:  0 : 10\nData loaded: 5000 / 5120     Remaining time:  0 : 5\nData loaded: 5100 / 5120     Remaining time:  0 : 0\nTrain data loaded: 5120\n \nTest data loaded: 100 / 2048     Remaining time:  0 : 57\nTest data loaded: 200 / 2048     Remaining time:  0 : 54\nTest data loaded: 300 / 2048     Remaining time:  0 : 51\nTest data loaded: 400 / 2048     Remaining time:  0 : 48\nTest data loaded: 500 / 2048     Remaining time:  0 : 45\nTest data loaded: 600 / 2048     Remaining time:  0 : 42\nTest data loaded: 700 / 2048     Remaining time:  0 : 39\nTest data loaded: 800 / 2048     Remaining time:  0 : 36\nTest data loaded: 900 / 2048     Remaining time:  0 : 33\nTest data loaded: 1000 / 2048     Remaining time:  0 : 30\nTest data loaded: 1100 / 2048     Remaining time:  0 : 27\nTest data loaded: 1200 / 2048     Remaining time:  0 : 24\nTest data loaded: 1300 / 2048     Remaining time:  0 : 21\nTest data loaded: 1400 / 2048     Remaining time:  0 : 18\nTest data loaded: 1500 / 2048     Remaining time:  0 : 15\nTest data loaded: 1600 / 2048     Remaining time:  0 : 12\nTest data loaded: 1700 / 2048     Remaining time:  0 : 9\nTest data loaded: 1800 / 2048     Remaining time:  0 : 6\nTest data loaded: 1900 / 2048     Remaining time:  0 : 3\nTest data loaded: 2000 / 2048     Remaining time:  0 : 0\nTest data loaded: 2048\n \nLabelled data loaded: 100 / 5120     Remaining time:  2 : 30\nLabelled data loaded: 200 / 5120     Remaining time:  2 : 27\nLabelled data loaded: 300 / 5120     Remaining time:  2 : 24\nLabelled data loaded: 400 / 5120     Remaining time:  2 : 21\nLabelled data loaded: 500 / 5120     Remaining time:  2 : 18\nLabelled data loaded: 600 / 5120     Remaining time:  2 : 15\nLabelled data loaded: 700 / 5120     Remaining time:  2 : 12\nLabelled data loaded: 800 / 5120     Remaining time:  2 : 9\nLabelled data loaded: 900 / 5120     Remaining time:  2 : 6\nLabelled data loaded: 1000 / 5120     Remaining time:  2 : 3\nLabelled data loaded: 1100 / 5120     Remaining time:  2 : 0\nLabelled data loaded: 1200 / 5120     Remaining time:  1 : 57\nLabelled data loaded: 1300 / 5120     Remaining time:  1 : 54\nLabelled data loaded: 1400 / 5120     Remaining time:  1 : 51\nLabelled data loaded: 1500 / 5120     Remaining time:  1 : 48\nLabelled data loaded: 1600 / 5120     Remaining time:  1 : 45\nLabelled data loaded: 1700 / 5120     Remaining time:  1 : 42\nLabelled data loaded: 1800 / 5120     Remaining time:  1 : 39\nLabelled data loaded: 1900 / 5120     Remaining time:  1 : 36\nLabelled data loaded: 2000 / 5120     Remaining time:  1 : 33\nLabelled data loaded: 2100 / 5120     Remaining time:  1 : 30\nLabelled data loaded: 2200 / 5120     Remaining time:  1 : 27\nLabelled data loaded: 2300 / 5120     Remaining time:  1 : 24\nLabelled data loaded: 2400 / 5120     Remaining time:  1 : 21\nLabelled data loaded: 2500 / 5120     Remaining time:  1 : 18\nLabelled data loaded: 2600 / 5120     Remaining time:  1 : 15\nLabelled data loaded: 2700 / 5120     Remaining time:  1 : 12\nLabelled data loaded: 2800 / 5120     Remaining time:  1 : 9\nLabelled data loaded: 2900 / 5120     Remaining time:  1 : 6\nLabelled data loaded: 3000 / 5120     Remaining time:  1 : 3\nLabelled data loaded: 3100 / 5120     Remaining time:  1 : 0\nLabelled data loaded: 3200 / 5120     Remaining time:  0 : 57\nLabelled data loaded: 3300 / 5120     Remaining time:  0 : 54\nLabelled data loaded: 3400 / 5120     Remaining time:  0 : 51\nLabelled data loaded: 3500 / 5120     Remaining time:  0 : 48\nLabelled data loaded: 3600 / 5120     Remaining time:  0 : 45\nLabelled data loaded: 3700 / 5120     Remaining time:  0 : 56\nLabelled data loaded: 3800 / 5120     Remaining time:  0 : 52\nLabelled data loaded: 3900 / 5120     Remaining time:  0 : 48\nLabelled data loaded: 4000 / 5120     Remaining time:  0 : 44\nLabelled data loaded: 4100 / 5120     Remaining time:  0 : 40\nLabelled data loaded: 4200 / 5120     Remaining time:  0 : 36\nLabelled data loaded: 4300 / 5120     Remaining time:  0 : 32\nLabelled data loaded: 4400 / 5120     Remaining time:  0 : 28\nLabelled data loaded: 4500 / 5120     Remaining time:  0 : 24\nLabelled data loaded: 4600 / 5120     Remaining time:  0 : 20\nLabelled data loaded: 4700 / 5120     Remaining time:  0 : 16\nLabelled data loaded: 4800 / 5120     Remaining time:  0 : 12\nLabelled data loaded: 4900 / 5120     Remaining time:  0 : 8\nLabelled data loaded: 5000 / 5120     Remaining time:  0 : 4\nLabelled data loaded: 5100 / 5120     Remaining time:  0 : 0\nLabelled data loaded: 5200 / 5120     Remaining time:  -1 : 56\nLabelled data loaded: 5300 / 5120     Remaining time:  -1 : 52\nLabelled data loaded: 5400 / 5120     Remaining time:  -1 : 48\nLabelled data loaded: 5500 / 5120     Remaining time:  -1 : 44\nLabelled data loaded: 5600 / 5120     Remaining time:  -1 : 40\nLabelled data loaded: 5700 / 5120     Remaining time:  -1 : 36\nLabelled data loaded: 5800 / 5120     Remaining time:  -1 : 32\nLabelled data loaded: 5900 / 5120     Remaining time:  -1 : 28\nLabelled data loaded: 6000 / 5120     Remaining time:  -1 : 24\nLabelled data loaded: 6100 / 5120     Remaining time:  -1 : 20\nLabelled data loaded: 6200 / 5120     Remaining time:  -1 : 16\nLabelled data loaded: 6233\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "586fcd78be4c8ec2b945248d87601cf271a9ee42"
      },
      "cell_type": "code",
      "source": "print(torch.sum(Target_train)/No_train_samples)\nprint(torch.sum(Target_test)/No_test_samples)\n\n\n# Show an example image \nim = Image[5]\nim = im.squeeze(0)\npylab.imshow(im, cmap=pylab.cm.gist_gray)\npylab.axis('off')\n\ntorch.cuda.max_memory_cached(device=0)\ntorch.cuda.memory_allocated(device = 0)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tensor(0.4396)\ntensor(0.3535)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bb84df76e774d89d86fe10741f9cf7e0748d531"
      },
      "cell_type": "code",
      "source": "# Histogram Equalization\n\"\"\"\nfrom IPython.display import Image\nfrom skimage import exposure\nimport pylab\n\nImage = []\nrow_int = []\n\nfor i in range(0,16):\n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect', anti_aliasing=True)) # resize image\n    tmp = equalize_hist(Image[-1])\n    row_int.append(np.mean(tmp[:,40:180],axis=1))\n\n# Plotting of images\nf, axarr = plt.subplots(1, 1, figsize=(50, 25))\n\ncolumns = 10\ncanvas = np.zeros((columns*IMG_SIZE, IMG_SIZE*3))\nfor j in range(columns):\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 0:IMG_SIZE] = Image[j]\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , IMG_SIZE:2*IMG_SIZE] = equalize_hist(Image[j])\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 2*IMG_SIZE:3*IMG_SIZE] = np.matrix.transpose(np.tile(row_int[j],(IMG_SIZE,1)))\n\nplt.imshow(canvas, cmap='gray')\nax = axarr\nax.set_title('Original Images')\nax.axis('off')\nax.grid(False)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05778375fb32b8b3a0a2e5b72822f6db8061e7fb",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "## Define size variables\nprint_shapes = False\nheight = IMG_SIZE\nwidth = IMG_SIZE\nchannels = 1\nnum_features = height*width*channels\n\n# Regulization\nL2_reg = 1e-6\nDROPOUT = True\ndo_p = 0.05 # do_p for conv \ndo_p2 = 0.1 # do_p for linear   NB: Classifier dropout is set manuel to 0.3\nbatchnorm_eps = 1e-5\nbatchnorm_momentum = 0.2\n\n\n# Conv Layers\nconv_out_channels = [8, 32, 64]\nconv_kernel = [5, 5, 3]\nconv_padding = [0, 2, 1]\nconv_stride = [1, 1, 1]\n\n# MaxPool Layers\npool_kernel = 3\npool_padding = 0\npool_stride = 3\n\n# Fully connected layers\nlin_layer = [1000, 200]\n\n# auxillary parameters\naux_layer = [200, 200]\naux_variables = 0\naux_in = 2 # layer no. where a is included in encoder\naux_decoder_layers = [200,200]\n\n# classifier parameters\nclassifier_layer = [1000,200]\nNo_classes = len(classes)\n\n# No. of layes\nNUM_CONV = len(conv_out_channels)\nNUM_LIN = len(lin_layer)\nNUM_AUX = len(aux_layer)\nNUM_CLASS = len(classifier_layer)\nNUM_AUX_DECODER = len(aux_decoder_layers)\n\n# Calculating the dimensions \ndef compute_conv_dim(height, width, kernel_size, padding_size, stride_size):\n    height_new = int((height - kernel_size + 2 * padding_size) / stride_size + 1)\n    width_new =  int((width  - kernel_size + 2 * padding_size) / stride_size + 1)\n    return [height_new, width_new]\n\ndef compute_final_dimension(height, width, last_num_channels, num_layers):\n    # First conv layer\n    CNN_height = height\n    CNN_width = width\n    for i in range(num_layers):\n        # conv layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, conv_kernel[i], conv_padding[i], conv_stride[i])\n        # maxpool layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, pool_kernel, pool_padding, pool_stride)\n    final_dim = CNN_height * CNN_width * last_num_channels\n    print(final_dim,CNN_height,CNN_width)\n    return [final_dim, CNN_height, CNN_width]\n\ndef normalize(x):\n    tmp = x-torch.min( torch.min(x,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0]\n    if torch.sum(torch.isnan(tmp))>0:\n        print(\"nan of tmp\",torch.sum(torch.isnan(tmp)))\n    return tmp/(torch.max( torch.max(tmp,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0] + 1e-8)  \n\ndef gaussian_sample(mu,log_var, num_samples, latent_features):    \n    # Don't propagate gradients through randomness\n    with torch.no_grad():\n        batch_size = mu.size(0)\n        epsilon = torch.randn(batch_size, num_samples, latent_features)\n            \n    if cuda:\n        epsilon = epsilon.cuda()\n        \n    sigma = torch.exp(log_var/2)\n        \n    # We will need to unsqueeze to turn\n    # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n    if len(mu.shape) == 2:\n        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)\n    else:\n        z = mu + epsilon * sigma\n    return z\n\ndef output_recon(x):\n    # Shape of x_mean: [batch_size, num_samples, channel, height, width]\n    x_mean, x_log_var = torch.chunk(x, 2, dim=2) # the mean and log_var reconstructions from the decoder\n    \n    # The original digits are on the scale [0, 1] \n    x_hat = x_mean[:,1,].unsqueeze(1)\n    #x_hat = normalize(x_mean[:,1,].unsqueeze(1))# to scale for showing an image\n    #x_hat = normalize(x_mean)\n    x_log_var = softplus(x_log_var)\n    \n    # Mean over samples\n    #x_hat = torch.mean(x_hat, dim=1)\n    x_log_var= torch.mean(x_log_var, dim=1)\n    x_mean = torch.mean(x_mean,dim=1) # used for the loss\n    \n    # Resize x_hat from [batch_size, no_features] to [batch_size, channels, height, width]\n    x_hat = x_hat.view( batch_size, 1, height, width)\n    x_log_var = x_log_var.view( batch_size, 1, height, width)\n    return x_hat, x_log_var, x_mean\n\n######## Image has to be: (num, channels, height, width)!!!! #########\nclass CNN_VAE(nn.Module):\n    def encoder(self,x):\n        # Convolutional layers of encoder\n        for i in range(0,len(self.Encoder_conv),3):\n            x = self.Encoder_conv[i](x) # Convolutional layer\n            self.layer_size.append(x.shape[-1])\n            x = self.Encoder_conv[i+1](x) # Batchnorm layer\n            x = relu(x)\n            if DROPOUT:\n                x = dropout2d(x, p=do_p)   \n            x = self.Encoder_conv[i+2](x) # Maxpool Layer\n        x = x.view(batch_size, -1) # Prepare x for linear layers\n        \n        # Fully connected layers of encoder\n        for i in range(0,len(self.Encoder_FC),2): \n            x = self.Encoder_FC[i](x) # Linear layer\n            x = self.Encoder_FC[i+1](x) # Batchnorm\n            x = relu(x)\n            if DROPOUT:\n                x = dropout(x, p=do_p2)\n        return x\n\n    \n    def decoder(self,z,y):\n        x = torch.cat([z,y],dim=-1)\n        # Fully connected layers of decoder\n        for i in range(0,len(self.Decoder_FC),2):\n            x = self.Decoder_FC[i](x)\n            x = x.permute(0,2,1)\n            x = self.Decoder_FC[i+1](x)\n            x = x.permute(0,2,1)\n            x = relu(x)\n            x = dropout(x,p= do_p2)\n        x = x.view(-1, self.Decoder_conv[0].in_channels, self.final_dim[1], self.final_dim[2])\n        \n        # Convolutional layers of decoder\n        curr_layer = len(self.Decoder_conv)//2-1\n        for i in range(0,len(self.Decoder_conv),2):\n            x = interpolate(x,size = [self.layer_size[curr_layer],self.layer_size[curr_layer]],\n                                      mode = 'bilinear', \n                                      align_corners = False)\n            curr_layer -=1\n            x = self.Decoder_conv[i](x) # Convolutional layers\n            x = self.Decoder_conv[i+1](x) # BatchNorm\n            x = relu(x)\n            if DROPOUT:\n                x = dropout2d(x, p=do_p)\n        return x.view(batch_size,-1,channels*2,height,width)\n            \n    def encoder_aux(self,a):\n        for i in range(0,len(self.Encoder_aux),2):\n            a = self.Encoder_aux[i](a)\n            a = self.Encoder_aux[i+1](a)\n            a = relu(a)\n            if DROPOUT:\n                a = dropout(a, p=do_p2)\n        q_a_mu, q_a_log_var = torch.chunk(a, 2, dim=-1) # divide to mu and sigma\n        return q_a_mu, q_a_log_var\n    \n    def decoder_aux(self,xz,y):\n        a = torch.cat([xz,y],dim=-1)\n        for i in range(0,len(self.Decoder_aux),2):\n            a = self.Decoder_aux[i](a)\n            a = a.permute(0,2,1)\n            a = self.Decoder_aux[i+1](a)\n            a = a.permute(0,2,1)\n            a = relu(a)\n            if DROPOUT:\n                a = dropout(a, p=do_p2)  \n        return a\n    \n    def additional_layer(self,xa,y):\n        z = torch.cat([xa,y],dim=-1)\n        z = self.Additional_layer[0](z)\n        z = z.permute(0,2,1)\n        z = self.Additional_layer[1](z)\n        z = z.permute(0,2,1)\n        z = relu(z)\n        if DROPOUT:\n            z = dropout(z, p=do_p2) \n        return z\n    \n    def classifier(self,xa):\n        for i in range(0,len(self.Classifier),2):\n            xa = self.Classifier[i](xa)\n            if i < len(self.Classifier)-1:\n                if aux_variables > 0:\n                    xa = xa.permute(0,2,1)\n                    xa = self.Classifier[i+1](xa)\n                    xa = xa.permute(0,2,1)\n                else:\n                    xa = self.Classifier[i+1](xa)\n                xa = relu(xa)\n                if DROPOUT:\n                    xa = dropout(xa, p=0.3)\n        return softmax(xa,dim=-1)\n    \n    def sample_y(self,batch_size,num_samples,no_classes,i):\n        tmp = Variable(torch.zeros(no_classes))\n        tmp[i] = 1\n        if cuda:\n            tmp = tmp.cuda()\n        return tmp.repeat(batch_size,num_samples,1)\n    \n    def sample_from_latent(self,x):\n        x_UL = []\n        for j in range(No_classes):\n            tmp = self.decoder(x.unsqueeze(1).repeat(1,num_samples,1), self.sample_y(batch_size,num_samples,No_classes,j))\n            x_UL.append(tmp)\n        x_hat, _, _ = output_recon(sum(x_UL))\n        return x_hat\n  \n    \n    def __init__(self, latent_features, num_samples):\n        super(CNN_VAE, self).__init__()\n        \n        self.latent_features = latent_features\n        self.num_samples = num_samples\n        \n        # Calculate final size of the CNN\n        self.final_dim = compute_final_dimension(height,width,conv_out_channels[-1],NUM_CONV)\n        \n        ## Convolutional layers of the encoder\n        input_channels = channels\n        Encoder_conv = nn.ModuleList()\n        for i in range(NUM_CONV):\n            Encoder_conv.append(Conv2d( in_channels=input_channels,\n                                            out_channels=conv_out_channels[i],\n                                            kernel_size=conv_kernel[i],\n                                            stride=conv_stride[i],\n                                            padding=conv_padding[i]))\n            Encoder_conv.append(BatchNorm2d(conv_out_channels[i], eps = batchnorm_eps, momentum = batchnorm_momentum))\n            Encoder_conv.append(MaxPool2d(  kernel_size=pool_kernel, \n                                        stride=pool_stride,\n                                        padding=pool_padding,\n                                        return_indices = False))\n            input_channels = conv_out_channels[i]\n        self.add_module(\"Encoder_conv\",Encoder_conv)\n        \n        # Fully connected layers of encoder\n        Encoder_FC = nn.ModuleList()\n        in_weights = self.final_dim[0]\n        for i in range(NUM_LIN):\n            Encoder_FC.append(Linear(in_features=in_weights, out_features=lin_layer[i]))\n            Encoder_FC.append(BatchNorm1d(lin_layer[i], eps = batchnorm_eps, momentum = batchnorm_momentum))\n            in_weights = lin_layer[i]\n        self.add_module(\"Encoder_FC\",Encoder_FC)\n        \n        # map to latent space\n        Additional_layer = nn.ModuleList()\n        Additional_layer.append(Linear(in_features=lin_layer[-1]+aux_variables+No_classes, out_features=latent_features*2))\n        Additional_layer.append(BatchNorm1d(latent_features*2, eps = batchnorm_eps, momentum = batchnorm_momentum))\n        self.add_module(\"Additional_layer\",Additional_layer)\n        \n        # Auxillary network\n        if aux_variables > 0:\n            # Auxillary encoder\n            Encoder_aux = nn.ModuleList()\n            in_weights = lin_layer[-1]\n            for i in range(NUM_AUX):\n                Encoder_aux.append(Linear(in_features=in_weights, out_features=aux_layer[i]))\n                Encoder_aux.append(BatchNorm1d(aux_layer[i], eps = batchnorm_eps, momentum = batchnorm_momentum))\n                in_weights = aux_layer[i]\n            Encoder_aux.append(Linear(in_features=aux_layer[-1], out_features=aux_variables*2))\n            Encoder_aux.append(BatchNorm1d(aux_variables*2, eps = batchnorm_eps, momentum = batchnorm_momentum))\n            self.add_module(\"Encoder_aux\", Encoder_aux)\n            # Auxillary decoder\n            Decoder_aux = nn.ModuleList()\n            for i in range(NUM_AUX_DECODER):\n                if i == 0:\n                    in_weights = self.latent_features + lin_layer[-1] + No_classes\n                else:\n                    in_weights = aux_decoder_layers[i-1]\n                Decoder_aux.append(Linear(in_features=in_weights, out_features=aux_decoder_layers[i]))\n                Decoder_aux.append(BatchNorm1d(aux_decoder_layers[i], eps = batchnorm_eps, momentum = batchnorm_momentum))\n            Decoder_aux.append(Linear(in_features=aux_decoder_layers[-1], out_features=aux_variables*2))\n            Decoder_aux.append(BatchNorm1d(aux_variables*2, eps = batchnorm_eps, momentum = batchnorm_momentum))\n            self.add_module(\"Decoder_aux\", Decoder_aux)    \n        \n        # Initialize fully connected layers from latent space to convolutional layers\n        Decoder_FC = nn.ModuleList()\n        Decoder_FC.append(Linear(in_features=latent_features+No_classes, out_features=lin_layer[-1]))\n        Decoder_FC.append(BatchNorm1d(lin_layer[-1], eps = batchnorm_eps, momentum = batchnorm_momentum))\n        for i in reversed(range(NUM_LIN)):\n            if i == 0:\n                out_weights = self.final_dim[0]\n            else:\n                out_weights = lin_layer[i-1]\n            Decoder_FC.append(Linear(in_features=lin_layer[i], out_features=out_weights))\n            Decoder_FC.append(BatchNorm1d(out_weights, eps = batchnorm_eps, momentum = batchnorm_momentum))\n        self.add_module(\"Decoder_FC\",Decoder_FC)\n        \n        # Convolutional layers of the decoder\n        Decoder_conv = nn.ModuleList()\n        for i in reversed(range(NUM_CONV)):\n            if i == 0:\n                output_channels = channels*2\n            else:\n                output_channels = conv_out_channels[i-1] \n            Decoder_conv.append(ConvTranspose2d(in_channels=conv_out_channels[i],\n                                                out_channels=output_channels,\n                                                kernel_size=conv_kernel[i],\n                                                stride=conv_stride[i],\n                                                padding=conv_padding[i]))\n            Decoder_conv.append(BatchNorm2d(output_channels, eps = batchnorm_eps, momentum = batchnorm_momentum))\n        self.add_module(\"Decoder_conv\",Decoder_conv)\n\n        # Fully connected layers from convolutional layers to classification\n        Classifier = nn.ModuleList()\n        if aux_variables > 0:\n            in_weights = lin_layer[-1]+aux_variables\n        else:\n            in_weights = lin_layer[-1]\n        for i in range(NUM_CLASS):\n            Classifier.append(Linear(in_features=in_weights, out_features=classifier_layer[i]))\n            Classifier.append(BatchNorm1d(classifier_layer[i], eps = 1e-4, momentum = batchnorm_momentum))\n            in_weights = classifier_layer[i]\n        Classifier.append(Linear(in_features=classifier_layer[-1], out_features = No_classes))\n        self.add_module(\"Classifier\", Classifier)\n        \n        \n        # Initialize weight of layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                nn.init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                \n### Forward ####\n    def forward(self, x, y=None):\n        outputs = {}\n        self.indices = []\n        self.layer_size = []\n        x = self.encoder(x)\n        if aux_variables > 0:\n            q_a_mu, q_a_log_var = self.encoder_aux(x)\n            q_a = gaussian_sample(q_a_mu,q_a_log_var,num_samples,aux_variables) # sample auxillary variables\n            outputs[\"q_a\"] = q_a # Assign to outputs\n            xa = torch.cat([x.unsqueeze(1).repeat(1,num_samples,1),q_a],dim=2) # Create combined vector of x and q_a\n        else:\n            xa = x\n        \n        # Run trough classifier\n        logits = self.classifier(xa)\n        \n        if aux_variables <= 0:\n            logits = logits.unsqueeze(1)\n            logits = logits.repeat(1,num_samples,1)\n            xa = xa.unsqueeze(1).repeat(1,num_samples,1) \n            \n        # Map x, y, a to latent space\n        if y is None:\n            x_UL = []\n            for j in range(No_classes):\n                z = self.additional_layer(xa,self.sample_y(batch_size,num_samples,No_classes,j))\n                activation = z\n                x_UL.append(z)\n            lat_in = sum(x_UL)\n            del x_UL\n        else:\n            lat_in = self.additional_layer(xa,y.unsqueeze(1).repeat(1,num_samples,1))\n#             activation = lat_in\n            \n        # Split into mu and log_var\n        mu, log_var = torch.chunk(lat_in, 2, dim=-1)\n        # Make sure that the log variance is positive\n        log_var = softplus(log_var)\n        # Sample from latent space\n        z = gaussian_sample(mu,log_var,num_samples,latent_features)\n                \n        # aux. decoder\n        if aux_variables > 0:\n            xz = torch.cat([z, x.unsqueeze(1).repeat(1,z.shape[1],1)],dim = -1)\n            if y is None:\n                a_UL = []\n                for j in range(No_classes):\n                    a_UL.append(self.decoder_aux(xz,self.sample_y(batch_size,num_samples,No_classes,j)))\n                a_log_var = []\n                a_mean = []\n                for j in range(No_classes):\n                    tmp1, tmp2 = torch.chunk(a_UL[j], 2, dim=-1) # the mean and log_var reconstructions from the decoder\n                    a_mean.append(tmp1)\n                    a_log_var.append(softplus(tmp2))\n                del a_UL, tmp1, tmp2\n            else:\n                a = self.decoder_aux(xz,y.unsqueeze(1).repeat(1,num_samples,1))\n                a_mean, a_log_var = torch.chunk(a, 2, dim=-1) # the mean and log_var reconstructions from the decoder\n                a_log_var = softplus(a_log_var)    \n            \n        # Decoder     \n        if y is None:\n            x_UL = []\n            for j in range(No_classes):\n                tmp = self.decoder(z, self.sample_y(batch_size,num_samples,No_classes,j))\n                x_UL.append(tmp)\n            x_log_var = []\n            x_mean = []\n            x_hat = []\n            for j in range(No_classes):\n                tmp1, tmp2, tmp3 = output_recon(x_UL[j])\n                x_hat.append(tmp1)\n                x_log_var.append(tmp2)\n                x_mean.append(tmp3)\n            del x_UL, tmp1, tmp2, tmp3\n        else:\n            x = self.decoder(z, y.unsqueeze(1).repeat(1,num_samples,1))\n            x_hat, x_log_var, x_mean = output_recon(x)\n        \n        # Assign variables\n        outputs[\"x_hat\"] = x_hat # This is used for visulizations only \n        outputs[\"z\"] = z\n        outputs[\"mu\"] = mu\n        outputs[\"log_var\"] = log_var\n        \n        # image recontructions (notice they are outputted as matrices)\n        outputs[\"x_mean\"] =  x_mean #x_hat  # mean reconstructions (for loss!!!)\n        outputs[\"x_log_var\"] = x_log_var #torch.reshape(x_log_var,(-1,height,width)) # log var reconstructions (for loss!!!)\n        \n        # auxillary outputs\n        if aux_variables > 0:            \n            outputs[\"q_a_mu\"] = q_a_mu\n            outputs[\"q_a_log_var\"] = q_a_log_var\n            outputs[\"p_a_mu\"] = a_mean\n            outputs[\"p_a_log_var\"] = a_log_var\n        \n        # classifier outputs \n        outputs[\"y_hat\"] = logits\n        \n        # Activation of latent features\n#         outputs[\"activation\"] = activation\n\n        return outputs\n\n# The number of samples used then initialising the VAE, \n# is number of samples drawn from the distribution\nnum_samples = 5\nlatent_features = 128\n\nnet = CNN_VAE(latent_features, num_samples)\nprint(net)\n# Transfer model to GPU ifavailable\nif cuda:\n    net = net.cuda()\n\n## test\nx = torch.randn(batch_size,1,width, height)\nx = Variable(x)\nif cuda:\n    x = x.cuda()\n    y = None\ny = net(x)\nprint(y['x_hat'][0].shape)\nprint('before: ',torch.cuda.memory_allocated(device=0))\nimport gc\n#del y,x\n# gc.collect()\nprint('after: ',torch.cuda.memory_allocated(device=0))\n\n#for parameter in net.parameters():\n#    print(parameter.shape)\n#epsilon = torch.randn(batch_size, latent_features).cuda\n#samples = torch.sigmoid(net.decoder(net.latent_to_CNN(epsilon))).detach()",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1024 4 4\nCNN_VAE(\n  (Encoder_conv): ModuleList(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n    (1): BatchNorm2d(8, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (5): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(64, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (8): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n  )\n  (Encoder_FC): ModuleList(\n    (0): Linear(in_features=1024, out_features=1000, bias=True)\n    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (2): Linear(in_features=1000, out_features=200, bias=True)\n    (3): BatchNorm1d(200, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n  )\n  (Additional_layer): ModuleList(\n    (0): Linear(in_features=202, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n  )\n  (Decoder_FC): ModuleList(\n    (0): Linear(in_features=130, out_features=200, bias=True)\n    (1): BatchNorm1d(200, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (2): Linear(in_features=200, out_features=1000, bias=True)\n    (3): BatchNorm1d(1000, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (4): Linear(in_features=1000, out_features=1024, bias=True)\n    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n  )\n  (Decoder_conv): ModuleList(\n    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (2): ConvTranspose2d(32, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (3): BatchNorm2d(8, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n    (4): ConvTranspose2d(8, 2, kernel_size=(5, 5), stride=(1, 1))\n    (5): BatchNorm2d(2, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n  )\n  (Classifier): ModuleList(\n    (0): Linear(in_features=200, out_features=1000, bias=True)\n    (1): BatchNorm1d(1000, eps=0.0001, momentum=0.2, affine=True, track_running_stats=True)\n    (2): Linear(in_features=1000, out_features=200, bias=True)\n    (3): BatchNorm1d(200, eps=0.0001, momentum=0.2, affine=True, track_running_stats=True)\n    (4): Linear(in_features=200, out_features=2, bias=True)\n  )\n)\ntorch.Size([64, 1, 112, 112])\nbefore:  3584830464\nafter:  3584830464\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:337: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd92a1ecff915e0aa69a9a2613ef672228917283"
      },
      "cell_type": "code",
      "source": "from torch.nn.functional import binary_cross_entropy\nfrom torch import optim\nimport math\n\ndef Gaussian_density(sample_img,mu_img,log_var_img):\n    c = - 0.5 * math.log(2 * math.pi)\n    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n    #density = c -  (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n    #print(\"Density:\",density)\n    #print(\"Density.shape:\", density.shape)\n    return torch.sum(density,dim = 1) # Sum over channels\n\ndef kl_a_calc(q_a,q_mu, q_log_var,p_mu, p_log_var):\n    # The function assumes: \n        # q_a has dimension: [batch_size,No_samples,latent_features]\n        # q_mu/log_var has dimension: [batch_size,latent_features]\n        # p_mu/log_var has dimension: [batch_size,No_samples,latent_features]\n        \n    p_mu      = p_mu.view(batch_size,num_samples,-1)\n    p_mu      = torch.mean(p_mu, dim = 1)\n    p_log_var = p_log_var.view(batch_size,num_samples,-1)\n    p_log_var = torch.mean(p_log_var, dim = 1)\n    \n    def log_gaussian(x, mu, log_var):\n        log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n        log_pdf = torch.sum(log_pdf, dim=1) # sum over each over the observations (mu + log_var*epsilon)\n        log_pdf = torch.sum(log_pdf,dim=1) # sum over q_a, i.e. latent features\n        return log_pdf\n\n    # put in middle dimension\n    q_mu      = q_mu.unsqueeze(1)\n    q_log_var = q_log_var.unsqueeze(1)\n    p_mu      = p_mu.unsqueeze_(1)\n    p_log_var = p_log_var.unsqueeze_(1)\n    # densities of each disitribution \n    qz = log_gaussian(q_a,q_mu,q_log_var)\n    pz = log_gaussian(q_a,p_mu,p_log_var)\n    # kl divergence\n    kl = qz - pz\n    \n    return kl\n\ndef ELBO_loss(sample_img, outputs, kl_warmup=None):\n    # Parameter in deterministic warmup for KL divergence\n    beta = 1 if kl_warmup is None else kl_warmup\n    \n    # Weighting kl's and likelihood\n    w1 = 0.5\n    w2 = 0.5\n    \n    if type(outputs['x_mean']) == list:\n        ELBO = []\n        kl_x = -0.5 * torch.sum(1 + outputs['log_var'] - outputs['mu']**2 - torch.exp(outputs['log_var']), dim=1)\n        kl_x = torch.sum(kl_x,dim=1) # sum over the features\n        for j in range(No_classes):\n            likelihood = Gaussian_density(sample_img, outputs['x_mean'][j], outputs['x_log_var'][j])\n            if aux_variables > 0:\n                kl_a = kl_a_calc(outputs[\"q_a\"],outputs[\"q_a_mu\"],outputs[\"q_a_log_var\"],outputs[\"p_a_mu\"][j],outputs[\"p_a_log_var\"][j])\n                kl = w1 * kl_x + (1 - w1) * kl_a\n            else:\n                kl_a = torch.Tensor([0])\n                kl = kl_x \n            likelihood = likelihood.view(batch_size, -1)\n            likelihood = torch.sum(likelihood, dim=1) # Sum over features (224x224 = 50,176)\n            ELBO.append(w2 * likelihood - (1 - w2) * beta * kl)\n        \n        L = torch.cat( (torch.unsqueeze(ELBO[0],1),torch.unsqueeze(ELBO[1],1)),dim =1 )\n        # Calculate entropy H(q(y|x)) and sum over all labels\n        logits = torch.mean(outputs['y_hat'],dim = 1)\n        \n        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1) \n        L = torch.sum(torch.mul(logits, L), dim=-1)\n        \n        # Equivalent to -U(x)\n        U = L - H\n        \n        #RMS_1 = torch.sqrt(torch.mean((sample_img - outputs['x_mean'][0])**2))\n        #RMS_2 = torch.sqrt(torch.mean((sample_img - outputs['x_mean'][1])**2))\n        #U = - torch.sum(RMS_1 + RMS_2)\n        #U = - ( torch.abs(sample_img - outputs['x_mean'][0])**2 + torch.abs(sample_img - outputs['x_mean'][0])**2)\n        \n        return -torch.mean(U), -torch.mean(H), -torch.mean(L),  (1 - w2) * beta * torch.mean(kl), -w2 * torch.mean(likelihood), w1*torch.mean(kl_x), (1-w1)*torch.mean(kl_a)\n    else:\n        likelihood = Gaussian_density(sample_img, outputs['x_mean'], outputs['x_log_var'])\n        kl_x = -0.5 * torch.sum(1 + outputs['log_var'] - outputs['mu']**2 - torch.exp(outputs['log_var']), dim=1)\n        if aux_variables > 0:\n            kl_a = kl_a_calc(outputs[\"q_a\"],outputs[\"q_a_mu\"],outputs[\"q_a_log_var\"],outputs[\"p_a_mu\"],outputs[\"p_a_log_var\"])\n            kl = w1 * torch.mean(kl_x) + (1 - w1) * torch.mean(kl_a)\n        else:\n            kl_a = torch.Tensor([0])\n            kl = torch.mean(kl_x)\n        likelihood = likelihood.view(batch_size, -1)\n        likelihood = torch.sum(likelihood, dim=1) # Sum over features (224x224 = 50,176)\n        ELBO = w2 * torch.mean(likelihood) - (1 - w2) * beta * kl    \n        # Notice minus sign as we want to maximise ELBO\n        return -ELBO, (1 - w2) * beta * kl, -w2 * torch.mean(likelihood), w1*torch.mean(kl_x), (1-w1)*torch.mean(kl_a)\n    \n    # Regularization error: \n    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n    # and prior p(z) = N(z | mu, sigma*I).\n    \n    # In the case of the KL-divergence between diagonal covariance Gaussian and \n    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n    # variance estimator of KL(q||p)\n    # Combining the two terms in the evidence lower bound objective (ELBO) \n    # mean over batch\n\n    \n\n# Define optimizer: The Adam optimizer works really well with VAEs.\noptimizer = optim.Adam(net.parameters(), lr=0.001)\nloss_function = ELBO_loss",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "7e691c43f146ec4fb7e8a5157b35d650ecb8fbef"
      },
      "cell_type": "code",
      "source": "from torch.autograd import Variable\nimport gc\ngc.collect()\ntorch.cuda.empty_cache\n\nx, y = next(iter(train_loader_labelled))\nu, _ = next(iter(train_loader))\n\ny_hot =  torch.zeros([batch_size,2], requires_grad=True)\nfor i in range(len(y)):\n    y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n\nx, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\nif cuda:\n    # They need to be on the same device and be synchronized.\n    x, y, y_hot = x.cuda(device=0), y.cuda(device=0), y_hot.cuda(device=0)\n    u = u.cuda(device=0)\n\noutputs = net(u)    \n#loss, kl, likelihood = loss_function(u,outputs)\nelbo_u, elbo_H, elbo_L, kl_u, likelihood_u, kl_u_x, kl_u_a= loss_function(u,outputs)\noutputs = net(x,y_hot)\n\nx_hat = outputs[\"x_hat\"]\nmu, log_var = outputs[\"mu\"], outputs[\"log_var\"]\nmu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\nz = outputs[\"z\"]\nlogits = outputs[\"y_hat\"]\ny_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\nclassification_loss = torch.sum(torch.abs(y_hot - logits))\n\n#loss, kl = loss_function(x, mu_img, log_var_img, torch.sum(mu,dim = 1), torch.sum(log_var,dim = 1))\nloss, kl, likelihood, kl_l_x, kl_l_a = loss_function(u,outputs)\nprint('mu:          ',mu.shape,torch.sum(torch.isnan(mu)))\nprint('log_var:     ',log_var.shape,torch.sum(torch.isnan(log_var)))\nprint('mu_img:      ',mu_img.shape,torch.sum(torch.isnan(mu_img)))\nprint('log_var_img: ',log_var_img.shape,torch.sum(torch.isnan(log_var_img)))\nprint('x:           ',x.shape,torch.sum(torch.isnan(x)))\nprint('x_hat:       ',x_hat.shape,torch.sum(torch.isnan(x_hat)))\nprint('z:           ',z.shape,torch.sum(torch.isnan(z)))\nprint('Total loss:  ',loss)\nprint('kl:          ',kl)\nprint('Class. loss: ',classification_loss)\nprint('Likelihood:  ',likelihood)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "mu:           torch.Size([64, 5, 128]) tensor(0, device='cuda:0')\nlog_var:      torch.Size([64, 5, 128]) tensor(0, device='cuda:0')\nmu_img:       torch.Size([64, 1, 112, 112]) tensor(0, device='cuda:0')\nlog_var_img:  torch.Size([64, 1, 112, 112]) tensor(0, device='cuda:0')\nx:            torch.Size([64, 1, 112, 112]) tensor(0, device='cuda:0')\nx_hat:        torch.Size([64, 1, 112, 112]) tensor(0, device='cuda:0')\nz:            torch.Size([64, 5, 128]) tensor(0, device='cuda:0')\nTotal loss:   tensor(8314.0459, device='cuda:0', grad_fn=<NegBackward>)\nkl:           tensor(0.9077, device='cuda:0', grad_fn=<MulBackward0>)\nClass. loss:  tensor(309.4504, device='cuda:0', grad_fn=<SumBackward0>)\nLikelihood:   tensor(8313.1377, device='cuda:0', grad_fn=<MulBackward0>)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b2e7e96bc9272e10b8dacfa083a630c9ae5968c"
      },
      "cell_type": "code",
      "source": "# Function to normalize a single image\ndef normalize_2(x):\n    # Input: [1, height, width]\n    x_shape = x.shape\n    x = x.view(1,-1)\n    x = x - torch.min(x)\n    x = x / (torch.max(x) + 1e-8)\n    if torch.sum(torch.isnan(x))>0:\n        print(\"nan of tmp\",torch.sum(torch.isnan(x)))\n    return x.view(x_shape)\n\n# Function to calculate balanced accuracy\ndef balanced_accuracy( logits, y_hot, Do_print=None):\n    # Input:\n    # logits: [batch_size, num_samples, no_classes]\n    # y_hot: [batch_size, num_samples, no_classes]\n    \n    logits = logits.cpu().detach()\n    y_hot = y_hot.cpu().detach()\n    \n    logits = logits.view(-1,2)\n    y_hot = y_hot.view(-1,2)\n    \n    TP = torch.sum(y_hot[:,0]*torch.round(logits[:,0]))   # True posistive\n    FP = torch.sum(y_hot[:,1]*torch.round(logits[:,0]))   # False positive\n    FN = torch.sum(y_hot[:,0]*torch.round(logits[:,1]))   # False negative\n    TN = torch.sum(y_hot[:,1]*torch.round(logits[:,1]))   # True negative\n\n    P = TP + FN\n    N = FP + TN\n    \n    if Do_print != None:\n        print(\"TP: \",TP)\n        print(\"FP: \",FP)\n        print(\"TN: \",TN)\n        print(\"FN: \",FN)\n        print(\"P: \",P)\n        print(\"N: \",N)\n\n    acc = torch.sum(TP/P + TN/N)/2\n        \n    return acc\n\n# Function to calculate balanced binary crossentropy\ndef balanced_binary_cross_entropy( logits, y_hot):\n    # Input:\n    # logits: [batch_size, num_samples, no_classes]\n    # y_hot: [batch_size, num_samples, no_classes]\n    \n    classWeight = torch.FloatTensor([torch.sum(y_hot[:,1,0])/torch.sum(y_hot[:,1,]), torch.sum(y_hot[:,1,1])/torch.sum(y_hot[:,1,])]).cuda(device=0)\n    class_loss_0 = 0\n    class_loss_1 = 0\n    \n    for i in range(0, batch_size):\n        tmp = torch.mean(y_hot, dim = 1)[i][0] \n        if tmp == 0:\n            class_loss_0 += torch.nn.functional.binary_cross_entropy(logits[i], y_hot[i])\n        else:\n            class_loss_1 += torch.nn.functional.binary_cross_entropy(logits[i], y_hot[i])\n        \n    #bal_binary_cross_entropy = classWeight[0]*class_loss_0 + classWeight[1]*class_loss_1\n    bal_binary_cross_entropy = (0.5/classWeight[0])*class_loss_0 + (0.5/classWeight[1])*class_loss_1\n        \n    return bal_binary_cross_entropy/batch_size\n\ndef balanced_accuracy_test( logits, y_hot, Do_print=None):\n    # Input:\n    # logits: [batch_size, num_samples, no_classes]\n    # y_hot: [batch_size, num_samples, no_classes]\n    \n    logits = logits.cpu().detach()\n    y_hot = y_hot.cpu().detach()\n    \n    logits = logits.view(-1,2)\n    y_hot = y_hot.view(-1,2)\n    \n    TP = torch.sum(y_hot[:,0]*torch.round(logits[:,0]))   # True posistive\n    FP = torch.sum(y_hot[:,1]*torch.round(logits[:,0]))   # False positive\n    FN = torch.sum(y_hot[:,0]*torch.round(logits[:,1]))   # False negative\n    TN = torch.sum(y_hot[:,1]*torch.round(logits[:,1]))   # True negative\n\n    P = TP + FN\n    N = FP + TN\n    \n    if Do_print != None:\n        print(\"TP: \",TP)\n        print(\"FP: \",FP)\n        print(\"TN: \",TN)\n        print(\"FN: \",FN)\n        print(\"P: \",P)\n        print(\"N: \",N)\n\n    acc = torch.sum(TP/P + TN/N)/2\n        \n    return acc, TP, FP, FN, TN, P, N",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "84f3af7ceb3dc3f3c84e6b8f64bf36920ce6c31b"
      },
      "cell_type": "code",
      "source": "## import warnings\n#warnings.filterwarnings(\"ignore\")\ntorch.cuda.empty_cache\n# Deleting variable Image and reload package Image\n%reset_selective -f \"^Image$\"\nfrom IPython.display import Image\nfrom sklearn.manifold import TSNE\nimport sys\n\n\nimport os\nfrom sklearn.decomposition import PCA\n\nnum_epochs = 301 # No_train_samples // batch_size\nbatch_epo_u = No_train_samples // batch_size\nbatch_epo_l = No_train_labelled_samples // batch_size\nbatch_per_epoch = batch_epo_u if batch_epo_u>batch_epo_l else batch_epo_l\n\n\ntmp_img = \"tmp_vae_out.png\"\nshow_sampling_points = False\nclasses = [0,1]\n\ntrain_loss, valid_loss = [], []\ntrain_kl, valid_kl = [], []\ntrain_likelihood, valid_likelihood = [], []\ntrain_classification, valid_classification = [], []\ntrain_acc, valid_acc = [], []\ntrain_L, train_H, train_kl_u_x, train_kl_u_a, train_elbo_l, train_kl_l_x, train_kl_l_a = [], [], [], [], [], [], []\nvalid_L, valid_H, valid_kl_u_x, valid_kl_u_a, valid_elbo_l, valid_kl_l_x, valid_kl_l_a = [], [], [], [], [], [], []\n# train_latent_loss, valid_latent_loss = [], []\ntotal_loss = []\n\n# Classification Loss\nalpha = 10 * (No_train_samples + No_train_labelled_samples) / No_train_labelled_samples   \nprint(\"Weight of classification loss (alpha): \", alpha)\n\n# Latent Loss\nbeta = 0   \n\n# Deterministic Warm-Up\ndeterministic_increment = 75  # The increment in the deterministic warmup. n = 10 adds 1/10 to kl_warmup for every epoch.\nt_max = 1 # Maximal weight of KL divergence \n\ndevice = torch.device(\"cuda:0\" if cuda else \"cpu\")\nprint(\"Using device:\", device)\n\n\nfor epoch in range(num_epochs):\n    batch_loss, batch_kl, batch_likelihood, batch_acc, batch_classification = [], [], [], [], []\n    batch_L, batch_H, batch_kl_u_x, batch_kl_u_a, batch_elbo_l, batch_kl_l_x, batch_kl_l_a = [], [], [], [], [], [], []\n#     batch_latent_loss = []\n    net.train()\n    \n    # Deterministic Warmup for KL divergence\n    #kl_warmup = 1\n    if epoch >= deterministic_increment:\n        kl_warmup = 1\n    else:\n        kl_warmup = epoch * 1/deterministic_increment            \n    print(\"KL Warm-Up: \", kl_warmup)\n    ###############################\n    \n    # Go through each batch in the training dataset using the loader\n    # Note that y is not necessarily known as it is here\n    count = 0\n    total_loss_batch = 0\n    for (x, y), (u, _) in zip(cycle(train_loader_labelled), train_loader):\n        y_hot =  torch.zeros([batch_size,2])\n        for i in range(len(y)):\n            y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n            \n        x, y, u, y_hot = Variable(x), Variable(y), Variable(u), Variable(y_hot)\n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y, y_hot = x.cuda(device=0), y.cuda(device=0), y_hot.cuda(device=0)\n            u = u.cuda(device=0)\n\n        count = count + 1\n        if not count % (batch_per_epoch/4):\n            print(\"Epoch:\", epoch, \"Batch:\", count,\"/\",batch_per_epoch)\n\n        #### Unlabelled\n        outputs = net(u)\n        elbo_u, elbo_H, elbo_L, kl_u, likelihood_u, kl_u_x, kl_u_a = loss_function(u, outputs, kl_warmup)\n        elbo_H = elbo_H.cpu().detach().numpy()\n        elbo_L = elbo_L.cpu().detach().numpy()\n        kl_u_x = kl_u_x.cpu().detach().numpy()\n        kl_u_a = kl_u_a.cpu().detach().numpy()\n        \n        #### Labelled\n        outputs = net(x,y_hot)\n        logits = outputs[\"y_hat\"]\n        y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n        \n        # Weight the classification loss for the two classes\n        classification_loss =  alpha * balanced_binary_cross_entropy( logits, y_hot)\n        acc = balanced_accuracy( logits, y_hot)\n        batch_acc.append(100*acc)\n        elbo_l, kl_l, likelihood_l, kl_l_x, kl_l_a = loss_function(x, outputs, kl_warmup)\n        kl_l_x = kl_l_x.cpu().detach().numpy()\n        kl_l_a = kl_l_a.cpu().detach().numpy()\n        \n        #### Latent lose\n#         rho = torch.tensor(0.05)\n#         act = outputs[\"activation\"]\n#         act = (act-torch.min(act)) / torch.max(act-torch.min(act))\n#         # Mean over batches (training set): from [batch_size, num_samples, num_hidden_units] -> [num_samples, num_hidden_units]\n#         rho_hat = torch.mean(torch.abs(act), dim = 0)\n#         # Sum over num_samples and num_hidden_units\n#         latent_loss = beta * torch.torch.sum(torch.abs(rho * torch.log( rho / rho_hat ) \n#                                           + (1 - rho) * torch.log( (1-rho) / (1-rho_hat)))).cuda(device=0)\n\n        \n        ### Combine losses \n        loss =  elbo_l + elbo_u + classification_loss #+ latent_loss # notice alpha has been moved to where classification loss is calculated\n        total_loss_batch += loss.item()\n        \n        ### Optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        elbo_l = elbo_l.cpu().detach().numpy()\n        batch_classification.append(classification_loss.item())\n        batch_loss.append(loss.item())\n        batch_kl.append(kl_u.item()+kl_l.item())\n        batch_likelihood.append(likelihood_u.item() + likelihood_l.item())\n        batch_L.append(elbo_L) # NOT elbo_l, but L from elbo_u\n        batch_H.append(elbo_H)\n        batch_kl_u_x.append(kl_u_x)\n        batch_kl_u_a.append(kl_u_a)\n        batch_elbo_l.append(elbo_l)\n        batch_kl_l_x.append(kl_l_x)\n        batch_kl_l_a.append(kl_l_a)\n#         latent_loss = latent_loss.cpu().detach().numpy()\n#         batch_latent_loss.append(latent_loss)\n    \n    train_classification.append(np.mean(batch_classification))\n    train_acc.append(np.mean(batch_acc))    \n    train_loss.append(np.mean(batch_loss))\n    train_kl.append(np.mean(batch_kl))\n    train_likelihood.append(np.mean(batch_likelihood))\n    total_loss.append(total_loss_batch / (batch_size*batch_per_epoch))\n    train_L.append(np.mean(batch_L))\n    train_H.append(np.mean(batch_H))\n    train_kl_u_x.append(np.mean(batch_kl_u_x))\n    train_kl_u_a.append(np.mean(batch_kl_u_a))\n    train_elbo_l.append(np.mean(batch_elbo_l))\n    train_kl_l_x.append(np.mean(batch_kl_l_x))\n    train_kl_l_a.append(np.mean(batch_kl_l_a))\n#     train_latent_loss.append(np.mean(batch_latent_loss))\n    \n    \n    # Evaluate, do not propagate gradients\n    with torch.no_grad():\n        net.eval()\n        \n        # Just load a single batch from the test loader\n        x, y = next(iter(test_loader))\n        \n        y_hot =  torch.zeros([batch_size,2])\n        for i in range(len(y)):\n            y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n        x, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y_hot = x.cuda(device=0), y_hot.cuda(device=0)\n        \n        #### Unlabelled\n        outputs = net(u)\n        elbo_u, elbo_H, elbo_L, kl_u, likelihood_u, kl_u_x, kl_u_a = loss_function(u, outputs, kl_warmup)\n        elbo_H = elbo_H.cpu().detach().numpy()\n        elbo_L = elbo_L.cpu().detach().numpy()\n        kl_u_x = kl_u_x.cpu().detach().numpy()\n        kl_u_a = kl_u_a.cpu().detach().numpy()\n        \n        #### Labelled\n        outputs = net(x,y_hot)\n        x_hat = outputs['x_hat']\n        logits = outputs[\"y_hat\"]\n        z = outputs[\"z\"]\n        \n        y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n        #acc = torch.sum(y_hot.view(-1,2) * logits.view(-1,2), dim = 1).cpu().detach().numpy()\n        #valid_acc.append(sum(acc > 0.5) / batch_size*num_samples)\n        acc = balanced_accuracy( logits, y_hot)\n        valid_acc.append(100*acc)\n        \n        # Weight the classification loss for the two classes\n        #classWeight = torch.FloatTensor([torch.sum(y_hot[:,1,0])/torch.sum(y_hot[:,1,]), torch.sum(y_hot[:,1,1])/torch.sum(y_hot[:,1,])]).cuda(device=0)\n        #classification_loss = alpha*torch.nn.functional.binary_cross_entropy(logits,y_hot, weight=classWeight)\n        #classification_loss = torch.sum(torch.abs(y_hot - logits))\n        #classification_loss = alpha*torch.nn.functional.binary_cross_entropy(logits,y_hot)\n        classification_loss = alpha * balanced_binary_cross_entropy( logits, y_hot)\n        # elbo, kl = loss_function(x_hat, x, mu, log_var)\n        elbo_l, kl_l, likelihood_l, kl_l_x, kl_l_a = loss_function(x, outputs, kl_warmup)\n        kl_l_x = kl_l_x.cpu().detach().numpy()\n        kl_l_a = kl_l_a.cpu().detach().numpy()\n        \n        # We save the latent variable and reconstruction for later use\n        # we will need them on the CPU to plot\n        x = x.to(\"cpu\")\n        x_hat = x_hat.to(\"cpu\")\n        z = z.detach().to(\"cpu\").numpy()\n        \n    \n        valid_classification.append(classification_loss.item())\n        valid_loss.append(elbo_l.item()+classification_loss.item()+elbo_u.item())\n        valid_kl.append(kl_l.item())\n        valid_likelihood.append(likelihood_l.item())\n        valid_L.append(elbo_L)\n        valid_H.append(elbo_H)\n        valid_kl_u_x.append(kl_u_x)\n        valid_kl_u_a.append(kl_u_a)\n        valid_elbo_l.append(elbo_l.item())\n        valid_kl_l_x.append(kl_l_x)\n        valid_kl_l_a.append(kl_l_a)\n        \n    \n    if epoch == 0:\n        continue\n    \n    if epoch % 10:\n        continue\n    \n    validation_from = 20\n    if epoch < validation_from:\n        VALIDATION = False\n    else:\n        VALIDATION = True\n    \n    # -- Plotting --\n    f, axarr = plt.subplots(5, 2, figsize=(20, 30))\n\n    # Loss\n    ax = axarr[0, 0]\n    ax.set_title(\"ELBO\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Error')\n\n    ax.plot(np.arange(epoch+1), train_loss, color=\"black\")\n    if VALIDATION:\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_loss[validation_from:len(valid_loss)], color=\"gray\", linestyle=\"--\")\n        ax.legend(['Training', 'Validation'])\n    else:\n        ax.legend(['Training'])\n        \n        \n    # Latent space\n    ax = axarr[0, 1]\n\n    ax.set_title('Latent space')\n    ax.set_xlabel('Dimension 1')\n    ax.set_ylabel('Dimension 2')\n    \n    if batch_size > 4:\n        rows = 4\n        columns = batch_size // rows if batch_size // rows < 8 else 8            \n    else:\n        rows = 2\n        columns = 2\n    \n    span = np.linspace(-4, 4, rows)\n    grid = np.dstack(np.meshgrid(span, span)).reshape(-1, 2)\n    \n    # If you want to use a dimensionality reduction method you can use\n    # for example PCA by projecting on two principal dimensions\n\n    \"\"\" PCA \"\"\"\n    #z = PCA(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    #z = z.reshape(batch_size,num_samples,2)\n    \n    \"\"\" TSNE \"\"\"\n    z = TSNE(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    z = z.reshape(batch_size,num_samples,2)\n    #z = z[:,1,] # We do only want to plot one z instead of |num_samples|\n    \n    colors = iter(plt.get_cmap('Set1')(np.linspace(0, 1.0, len(classes))))\n    for c in classes:\n        ax.scatter(*z[c == y.numpy()].reshape(-1, 2).T, c=next(colors), marker='o', label=c)\n        \n    if show_sampling_points:\n        ax.scatter(*grid.T, color=\"k\", marker=\"x\", alpha=0.5, label=\"Sampling points\")\n\n    ax.legend()\n    \n    # KL / reconstruction\n    ax = axarr[1, 0]\n    \n    ax.set_title(\"Losses\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n\n    ax.set_title(\"ELBO: Labelled and unlabelled\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')        \n    if VALIDATION:\n        ax.plot(np.arange(epoch+1), train_elbo_l, color=\"red\")\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_elbo_l[validation_from:len(valid_loss)], color=\"red\", linestyle=\"--\")\n        ax.plot(np.arange(epoch+1), train_L, color=\"green\")  \n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_L[validation_from:len(valid_loss)], color=\"green\", linestyle=\"--\") \n        ax.legend(['Train. ELBO Labelled',  'Valid. ELBO Labelled', 'Train. ELBO Unlabelled', 'Valid. ELBO Unlabelled'])\n    else:\n        ax.plot(np.arange(epoch+1), train_elbo_l, color=\"red\")\n        ax.plot(np.arange(epoch+1), train_L, color=\"green\")    # Typically L >>> H, therefore not L+H   \n        ax.legend(['Train. ELBO Labelled', 'Train. ELBO Unlabelled'])\n    \n    # Latent space samples\n    ax = axarr[1, 1]\n    ax.set_title('Samples from latent space')\n    ax.axis('off')\n\n    with torch.no_grad():\n        epsilon = torch.randn(batch_size, latent_features).to(device)\n        samples = torch.sigmoid(net.sample_from_latent(epsilon)).cpu().detach().numpy()\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = samples[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = samples[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Inputs\n    ax = axarr[2, 0]\n    ax.set_title('Inputs')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = x[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[2, 1]\n    ax.set_title('Reconstructions: mean')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x_hat[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = normalize_2(x_hat[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE)))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[3, 0]\n    ax.axis('off')\n    #ax.set_title('Reconstructions: sigma')\n    #sigma = (torch.exp(outputs[\"x_log_var\"]/2)).detach().to(\"cpu\").numpy()\n    #canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    #for i in range(rows):\n    #    for j in range(columns):\n    #        idx = i % columns + rows * j\n    #        #temp_img = x_hat[idx].reshape((224, 224))\n    #        #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n    #        canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = sigma[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    #ax.imshow(canvas, cmap='gray')\n    ax.set_title('Displaying latent features')\n    with torch.no_grad():\n        latent_feature = torch.zeros(batch_size,latent_features).to(device)\n        for i in range(0,latent_features if latent_features<= batch_size else batch_size):\n            latent_feature[i,i] = 100\n        displaying_latent_features = (net.sample_from_latent(latent_feature)).cpu().detach() #no sigmoid, instead normalize later\n        \n    if displaying_latent_features.shape[0] == 64: \n        rows_2 = 8\n        columns_2 = 8\n    elif displaying_latent_features.shape[0] == 128: \n        rows_2 = 8\n        columns_2 = 16\n    else:\n        rows_2 = rows\n        columns_2 = columns   \n    canvas = np.zeros((IMG_SIZE*rows_2, columns_2*IMG_SIZE))  \n    for i in range(rows_2):\n        for j in range(columns_2):\n            idx = i % columns_2 + rows_2 * j\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = normalize_2(displaying_latent_features[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE)))\n    ax.imshow(canvas, cmap='gray')\n    \n                \n    # Classification Loss\n    ax = axarr[3, 1]\n    ax.set_title(\"Classification Accuracy\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Accuracy (%)')\n    \n    ax.plot(np.arange(epoch+1), train_acc, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_acc, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n    \n    # KL Divergence - Unlabelled\n    ax = axarr[4,0]\n    ax.set_title(\"KL Divergence - Unlabelled\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    if VALIDATION:\n        ax.plot(np.arange(epoch+1), train_kl_u_x, color=\"blue\")\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_kl_u_x[validation_from:len(valid_loss)], color=\"blue\", linestyle=\"--\")\n        ax.plot(np.arange(epoch+1), train_kl_u_a, color=\"orange\")\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_kl_u_a[validation_from:len(valid_loss)], color=\"orange\", linestyle=\"--\")\n        ax.legend(['Train. kl_u_x',  'Valid. kl_u_x', 'Train. kl_u_a', 'Valid. kl_u_a'])\n    else:\n        ax.plot(np.arange(epoch+1), train_kl_u_x, color=\"blue\")\n        ax.plot(np.arange(epoch+1), train_kl_u_a, color=\"orange\")\n        ax.legend(['Train. kl_u_x',  'Train. kl_u_a'])\n\n    # KL Divergence - Labelled\n    ax = axarr[4,1]\n    ax.set_title(\"KL Divergence - Labelled\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    if VALIDATION:\n        ax.plot(np.arange(epoch+1), train_kl_l_x, color=\"blue\")\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_kl_l_x[validation_from:len(valid_loss)], color=\"blue\", linestyle=\"--\")\n        ax.plot(np.arange(epoch+1), train_kl_l_a, color=\"orange\")\n        ax.plot(np.arange(epoch+1)[validation_from:len(valid_loss)], valid_kl_l_a[validation_from:len(valid_loss)], color=\"orange\", linestyle=\"--\")\n        #ax.plot(np.arange(epoch+1), train_latent_loss, color=\"green\")\n        ax.legend(['Train. kl_l_x',   'Valid. kl_l_x', 'Train. kl_l_a', 'Valid. kl_l_a'])\n        #ax.legend(['Train. kl_l_x',   'Valid. kl_l_x', 'Train. kl_l_a', 'Valid. kl_l_a', 'Latent_loss'])\n    else:\n        ax.plot(np.arange(epoch+1), train_kl_l_x, color=\"blue\")\n        ax.plot(np.arange(epoch+1), train_kl_l_a, color=\"orange\")\n        #ax.plot(np.arange(epoch+1), train_latent_loss, color=\"green\")\n        ax.legend(['Train. kl_l_x',  'Train. kl_l_a'])\n        #ax.legend(['Train. kl_l_x',  'Train. kl_l_a',  'Latent_loss'])\n    \n    clear_output(wait=True)\n    plt.savefig(tmp_img)\n    plt.close(f)\n    display(Image(filename=tmp_img))\n\n    os.remove(tmp_img)\n    \n    if epoch % 20:\n        continue\n    \n    # Run trough full test data set\n    TP, FP, FN, TN, P, N = 0, 0, 0, 0, 0, 0\n    for (x, y) in test_loader:\n        y_hot =  torch.zeros([batch_size,2])\n\n        for i in range(len(y)):\n            y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n        x, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y_hot = x.cuda(device=0), y_hot.cuda(device=0)\n\n        #### Labelled\n        outputs = net(x,y_hot)\n        logits = outputs[\"y_hat\"]\n        y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n        acc_1, TP_1, FP_1, FN_1, TN_1, P_1, N_1 = balanced_accuracy_test(logits, y_hot)\n        TP += TP_1\n        FP += FP_1\n        FN += FN_1\n        TN += TN_1\n        P  += P_1\n        N  += N_1\n\n    acc =  torch.sum(TP/P + TN/N)/2  \n    print(\"Acc: \",acc)    \n    print(\"TP: \",TP)\n    print(\"FP: \",FP)\n    print(\"TN: \",TN)\n    print(\"FN: \",FN)\n    print(\"P: \",P)\n    print(\"N: \",N)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Weight of classification loss (alpha):  20.0\nUsing device: cuda:0\nKL Warm-Up:  0.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "6fcf669e3e61e9338cd6df370bdac88f2a0d4688"
      },
      "cell_type": "code",
      "source": "# Run trough full test data set\nTP, FP, FN, TN, P, N = 0, 0, 0, 0, 0, 0\nfor (x, y) in test_loader:\n    y_hot =  torch.zeros([batch_size,2])\n    \n    for i in range(len(y)):\n        y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n    x, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\n    if cuda:\n        # They need to be on the same device and be synchronized.\n        x, y_hot = x.cuda(device=0), y_hot.cuda(device=0)\n\n    #### Labelled\n    outputs = net(x,y_hot)\n    logits = outputs[\"y_hat\"]\n    y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n    acc_1, TP_1, FP_1, FN_1, TN_1, P_1, N_1 = balanced_accuracy_test(logits, y_hot)\n    TP += TP_1\n    FP += FP_1\n    FN += FN_1\n    TN += TN_1\n    P  += P_1\n    N  += N_1\n\nacc =  torch.sum(TP/P + TN/N)/2  \nprint(\"Acc: \",acc)    \nprint(\"TP: \",TP)\nprint(\"FP: \",FP)\nprint(\"TN: \",TN)\nprint(\"FN: \",FN)\nprint(\"P: \",P)\nprint(\"N: \",N)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}